{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\nthe validity of the results.\nQ2. What are the three types of ANOVA, and in what situations would each be used?\nQ3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\nQ4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\nsum of squares (SSR) in a one-way ANOVA using Python?\nQ5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\nQ6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\nWhat can you conclude about the differences between the groups, and how would you interpret these\nresults?\nQ7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\nconsequences of using different methods to handle missing data?\nQ8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\nan example of a situation where a post-hoc test might be necessary.\nQ9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\nto determine if there are any significant differences between the mean weight loss of the three diets.\nReport the F-statistic and p-value, and interpret the results.\nQ10. A company wants to know if there are any significant differences in the average time it takes to\ncomplete a task using three different software programs: Program A, Program B, and Program C. They\nrandomly assign 30 employees to one of the programs and record the time it takes each employee to\ncomplete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\ninteraction effects between the software programs and employee experience level (novice vs.\nexperienced). Report the F-statistics and p-values, and interpret the results.\nQ11. An educational researcher is interested in whether a new teaching method improves student test\nscores. They randomly assign 100 students to either the control group (traditional teaching method) or the\nexperimental group (new teaching method) and administer a test at the end of the semester. Conduct a\ntwo-sample t-test using Python to determine if there are any significant differences in test scores\nbetween the two groups. If the results are significant, follow up with a post-hoc test to determine which\ngroup(s) differ significantly from each other.\nQ12. A researcher wants to know if there are any significant differences in the average daily sales of three\nretail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\non those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n\nsignificant differences in sales between the three stores. If the results are significant, follow up with a post-\nhoc test to determine which store(s) differ significantly from each other.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Solution\n\nLet's address each of your questions regarding ANOVA step by step:\n\n### Q1. Assumptions of ANOVA and Violations\nThe assumptions of ANOVA (Analysis of Variance) include:\n1. **Independence:** Observations are independent within and between groups.\n2. **Normality:** The dependent variable follows a normal distribution within each group.\n3. **Homogeneity of Variances:** The variances of the dependent variable are equal across all groups.\n\nViolations of these assumptions can impact the validity of ANOVA results:\n- **Independence Violation:** Data from repeated measures or nested designs can violate independence.\n- **Normality Violation:** Skewed or non-normally distributed data can lead to biased results, especially with small sample sizes.\n- **Homogeneity of Variances Violation:** Unequal variances can affect the F-statistic and p-values, making them less reliable.\n\n### Q2. Types of ANOVA and Situations\nThe three types of ANOVA are:\n1. **One-Way ANOVA:** Compares means of three or more independent groups on one factor variable.\n2. **Two-Way ANOVA:** Analyzes the interaction effects between two independent variables (factors).\n3. **Repeated Measures ANOVA:** Compares means of the same group under different conditions or time points.\n\nEach type is used based on the research design:\n- One-Way ANOVA for comparing means across multiple groups.\n- Two-Way ANOVA for analyzing the effects of two factors and their interaction.\n- Repeated Measures ANOVA for within-subject designs or repeated measurements.\n\n### Q3. Partitioning of Variance in ANOVA\nThe partitioning of variance in ANOVA decomposes the total variability into different components:\n- **Total Sum of Squares (SST):** Variation in the dependent variable across all observations.\n- **Explained Sum of Squares (SSE):** Variation explained by the factors or independent variables.\n- **Residual Sum of Squares (SSR):** Variation not explained by the factors (error variance).\n\nUnderstanding this concept is crucial as it helps identify the sources of variability and assess the significance of factors or interactions.\n\n### Q4. Calculation of SST, SSE, and SSR in One-Way ANOVA using Python\nIn Python, you can calculate SST, SSE, and SSR in a One-Way ANOVA using libraries like `scipy.stats` or `statsmodels`.\n\nFor example:\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Sample data for groups A, B, and C\ndata_A = [10, 12, 15, 18, 20]\ndata_B = [8, 11, 14, 16, 19]\ndata_C = [9, 13, 16, 17, 21]\n\n# Combine data into one array\nall_data = np.concatenate([data_A, data_B, data_C])\n\n# Calculate means\nmean_total = np.mean(all_data)\nmean_A = np.mean(data_A)\nmean_B = np.mean(data_B)\nmean_C = np.mean(data_C)\n\n# Calculate Total Sum of Squares (SST)\nSST = np.sum((all_data - mean_total) ** 2)\n\n# Calculate Explained Sum of Squares (SSE)\nSSE = len(data_A) * (mean_A - mean_total) ** 2 + len(data_B) * (mean_B - mean_total) ** 2 + len(data_C) * (mean_C - mean_total) ** 2\n\n# Calculate Residual Sum of Squares (SSR)\nSSR = SST - SSE\n\nprint(\"Total Sum of Squares (SST):\", SST)\nprint(\"Explained Sum of Squares (SSE):\", SSE)\nprint(\"Residual Sum of Squares (SSR):\", SSR)\n```\n\n### Q5. Calculation of Main Effects and Interaction Effects in Two-Way ANOVA using Python\nIn Two-Way ANOVA, main effects are the effects of each independent variable, while interaction effects are the combined effects of variables.\n\nYou can calculate main effects and interaction effects using ANOVA models from libraries like `statsmodels` or `scipy.stats`.\n\nHere's an example using `statsmodels`:\n```python\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Sample data for two factors (A and B) and dependent variable Y\ndata = {'A': [1, 1, 2, 2, 3, 3],\n        'B': [1, 2, 1, 2, 1, 2],\n        'Y': [10, 12, 15, 18, 20, 8]}\n\ndf = pd.DataFrame(data)\n\n# Fit ANOVA model\nmodel = ols('Y ~ A + B + A:B', data=df).fit()\n\n# Calculate main effects and\n\ninteraction effects\n```python\n# Calculate main effects and interaction effects\nmain_effects = model.params[['A', 'B']]\ninteraction_effect = model.params['A:B']\n\nprint(\"Main Effects:\")\nprint(main_effects)\nprint(\"Interaction Effect:\")\nprint(interaction_effect)\n```\n\n### Q6. Interpretation of One-Way ANOVA Results (F-statistic and p-value)\nFor a one-way ANOVA where you obtained an F-statistic of 5.23 and a p-value of 0.02:\n\n- **F-statistic:** Indicates the ratio of explained variance to unexplained variance. A higher F-value suggests more significant differences between group means.\n- **P-value:** Represents the probability of obtaining the observed F-statistic if the null hypothesis (no group differences) were true. A p-value below the significance level (e.g., 0.05) indicates significant differences.\n\nIn this case:\n- F-statistic of 5.23 suggests there are significant differences between the groups.\n- P-value of 0.02 (less than 0.05) indicates that the observed differences are unlikely to be due to random chance.\n\nTherefore, you would conclude that there are statistically significant differences between the groups.\n\n### Q7. Handling Missing Data in Repeated Measures ANOVA\nIn a repeated measures ANOVA, missing data can be handled using various methods:\n1. **Complete Case Analysis:** Exclude cases with missing data (listwise deletion).\n2. **Mean Imputation:** Replace missing values with the mean of the observed values.\n3. **Last Observation Carried Forward (LOCF):** Use the last observed value for missing data.\n4. **Interpolation:** Estimate missing values based on neighboring data points.\n5. **Multiple Imputation:** Generate multiple imputed datasets to account for uncertainty.\n\nThe consequences of using different methods include:\n- Complete case analysis may lead to loss of statistical power if missing data are not random.\n- Mean imputation can underestimate variability and bias results.\n- LOCF may introduce bias if data are not missing completely at random.\n- Interpolation methods rely on assumptions about data continuity.\n- Multiple imputation can provide more robust estimates but requires more complex analysis.\n\n### Q8. Common Post-hoc Tests after ANOVA and Usage\nCommon post-hoc tests after ANOVA include:\n1. **Tukey's Honestly Significant Difference (HSD):** Compares all pairs of group means to determine which pairs are significantly different.\n2. **Bonferroni Correction:** Adjusts significance levels for multiple comparisons to control Type I error rate.\n3. **Duncan's Test:** Similar to Tukey's HSD but less conservative.\n4. **Scheff√©'s Test:** Suitable for unequal sample sizes and complex comparisons.\n\nYou would use post-hoc tests when ANOVA indicates significant differences among group means but does not specify which groups differ significantly. For example, after conducting a one-way ANOVA comparing mean weight loss among three diets, you might use Tukey's HSD to identify specific pairwise differences.\n\n\n\n### Q9. One-Way ANOVA to Compare Mean Weight Loss of Three Diets\n\n```python\nimport scipy.stats as stats\n\n# Sample data for weight loss in three diets: A, B, and C\ndata = {'Diet': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n        'Weight Loss': [5, 4, 6, 3, 2, 4, 6, 5, 7]}\n\ndf = pd.DataFrame(data)\n\n# Perform one-way ANOVA\nf_statistic, p_value = stats.f_oneway(df[df['Diet'] == 'A']['Weight Loss'],\n                                      df[df['Diet'] == 'B']['Weight Loss'],\n                                      df[df['Diet'] == 'C']['Weight Loss'])\n\nprint(\"F-statistic:\", f_statistic)\nprint(\"P-value:\", p_value)\n\n# Interpretation\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: There are significant differences in mean weight loss among the three diets.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There are no significant differences in mean weight loss among the three diets.\")\n```\n\n### Q10. Two-Way ANOVA for Software Programs and Employee Experience Level\n\n```python\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Sample data for software programs, employee experience, and task completion time\ndata = {'Software': ['A', 'A', 'B', 'B', 'C', 'C'] * 5,\n        'Experience': ['Novice'] * 15 + ['Experienced'] * 15,\n        'Time': [10, 12, 11, 13, 9, 11, 14, 15, 13, 12,\n                 8, 10, 9, 11, 10, 13, 12, 11, 9, 10,\n                 16, 18, 17, 19, 15, 16, 18, 17, 16, 15]}\n\ndf = pd.DataFrame(data)\n\n# Fit Two-Way ANOVA model\nmodel = ols('Time ~ Software * Experience', data=df).fit()\n\n# Perform ANOVA\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(anova_table)\n```\n\nThe output will provide F-statistics and p-values for main effects (Software, Experience) and interaction effects (Software * Experience).\n\n### Q11. Two-Sample T-Test for Test Scores\n\n```python\nfrom scipy.stats import ttest_ind\n\n# Sample data for control and experimental groups' test scores\ncontrol_scores = [80, 85, 82, 78, 86, 84, 83, 79, 81, 87]\nexperimental_scores = [75, 79, 77, 73, 78, 76, 74, 80, 72, 81]\n\n# Perform two-sample t-test\nt_statistic, p_value = ttest_ind(control_scores, experimental_scores)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"P-value:\", p_value)\n\n# Interpretation\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: There are significant differences in test scores between the control and experimental groups.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There are no significant differences in test scores between the control and experimental groups.\")\n```\n\n### Q12. Repeated Measures ANOVA for Daily Sales of Retail Stores\n\nFor conducting a repeated measures ANOVA and post-hoc tests, you can use libraries like `statsmodels` or `pingouin`.\n\nHere's an example using `pingouin` for repeated measures ANOVA and pairwise comparisons:\n\n```python\nimport pingouin as pg\n\n# Sample data for daily sales of three retail stores: A, B, and C\ndata = {'Day': np.repeat(np.arange(30), 3),\n        'Store': np.tile(['A', 'B', 'C'], 30),\n        'Sales': np.random.randint(50, 100, size=90)}\n\ndf = pd.DataFrame(data)\n\n# Repeated Measures ANOVA\nrm_anova = pg.rm_anova(data=df, dv='Sales', within='Store', subject='Day')\nprint(rm_anova)\n\n# Post-hoc pairwise comparisons\nposthoc = pg.pairwise_ttests(data=df, dv='Sales', within='Store', subject='Day', padjust='holm')\nprint(posthoc)\n```\n\nThis code performs a repeated measures ANOVA on the daily sales data of three retail stores and then conducts post-hoc pairwise comparisons using the Holm correction method.\n\nLet me know if you need further clarification or assistance with any part!",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}