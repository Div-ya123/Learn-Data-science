{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Q1. What is the mathematical formula for a linear SVM?\nQ2. What is the objective function of a linear SVM?\nQ3. What is the kernel trick in SVM?\nQ4. What is the role of support vectors in SVM Explain with example\nQ5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\nSVM?\nQ6. SVM Implementation through Iris dataset.\n~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n~ Compute the accuracy of the model on the testing setl\n~ Plot the decision boundaries of the trained model using two of the featuresl\n~ Try different values of the regularisation parameter C and see how it affects the performance of\nthe model.\nBonus task: Implement a linear SVM classifier from scratch using Python and compare its\nperformance with the scikit-learn implementation.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n**Q1. What is the mathematical formula for a linear SVM?**\n\nThe mathematical formula for a linear Support Vector Machine (SVM) is given by:\n\n\\[ f(x) = w^T x + b \\]\n\nwhere:\n- \\( f(x) \\) is the decision function that assigns input \\( x \\) to one of the two classes (e.g., -1 or +1).\n- \\( w \\) is the weight vector.\n- \\( x \\) is the input feature vector.\n- \\( b \\) is the bias term.\n\nIn a binary classification problem, the decision function output \\( f(x) \\) determines the class label based on whether it is greater than or less than zero.\n\n**Q2. What is the objective function of a linear SVM?**\n\nThe objective function of a linear SVM is to maximize the margin between the two classes while minimizing the classification error. Mathematically, it can be expressed as:\n\n\\[ \\min_{w,b} \\frac{1}{2} \\|w\\|^2 \\]\n\nsubject to the constraints:\n\n\\[ y_i(w^T x_i + b) \\geq 1 \\text{ for } i = 1, 2, ..., n \\]\n\nwhere:\n- \\( w \\) is the weight vector.\n- \\( b \\) is the bias term.\n- \\( x_i \\) is the \\( i \\)th feature vector.\n- \\( y_i \\) is the class label of the \\( i \\)th sample (either -1 or +1).\n- \\( n \\) is the number of samples.\n\n**Q3. What is the kernel trick in SVM?**\n\nThe kernel trick in SVM allows the algorithm to implicitly map input data into a higher-dimensional feature space without actually computing the transformation explicitly. This is achieved by defining a kernel function \\( K(x, x') \\) that computes the dot product of the mapped feature vectors in the higher-dimensional space. The kernel function allows SVM to efficiently operate in this higher-dimensional space without the need to compute the transformed feature vectors explicitly.\n\nCommonly used kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n\n**Q4. What is the role of support vectors in SVM? Explain with an example**\n\nSupport vectors are the data points that lie closest to the decision boundary (hyperplane) between the two classes. They play a crucial role in defining the decision boundary because the position of the hyperplane is determined by these support vectors.\n\nIn an example, consider a simple binary classification problem with two classes, represented by blue and red dots in a two-dimensional space. The decision boundary (hyperplane) is determined by the support vectors, which are the points closest to the boundary. These support vectors define the maximum margin between the two classes, and any changes in their position would affect the decision boundary.\n\n**Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?**\n\n- **Hyperplane:** In SVM, a hyperplane is a decision boundary that separates the classes. In a two-dimensional space, a hyperplane is a straight line. In higher-dimensional spaces, it is a flat affine subspace.\n- **Margin:** The margin is the distance between the hyperplane and the closest data points from each class, known as support vectors.\n- **Soft margin:** In soft-margin SVM, the margin is allowed to have some violations (misclassified points) to improve generalization on noisy datasets. It introduces a penalty term for misclassifications into the objective function.\n- **Hard margin:** In hard-margin SVM, no data points are allowed to violate the margin, meaning there should be no misclassifications. This may lead to overfitting if the data is noisy or not linearly separable.\n\nBelow are the graphical representations:\n\nHard Margin SVM:\n![Hard Margin SVM](https://miro.medium.com/max/1200/1*XOkho2_eZqrCmW1h_2jEXg.png)\n\nSoft Margin SVM:\n![Soft Margin SVM](https://miro.medium.com/max/700/1*9nS3rkNFV9ORHw4Nxe3ABw.png)\n\n**Q6. SVM Implementation through the Iris dataset:**\n\n```python\n# Import necessary libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data[:, :2]  # Use only first two features for visualization\ny = iris.target\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a linear SVM classifier on the training set\nsvm_classifier = SVC(kernel='linear', C=1)\nsvm_classifier.fit(X_train, y_train)\n\n# Predict the labels for the testing set\ny_pred = svm_classifier.predict(X_test)\n\n# Compute the accuracy of the model on the testing set\naccuracy = np.mean(y_pred == y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Plot the decision boundaries of the trained model using two of the features\nplt.figure(figsize=(10, 6))\n\n# Plot the data points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', s=80, label='Data points')\n\n# Plot the decision boundaries\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create grid to evaluate model\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\nYY, XX = np.meshgrid(yy, xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = svm_classifier.decision_function(xy).reshape(XX.shape)\n\n# Plot decision boundary and margins\nax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\nax.scatter(svm_classifier.support_vectors_[:, 0], svm_classifier.support_vectors_[:, 1], s=100,\n           linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.title('Decision Boundaries of Linear SVM on Iris Dataset')\nplt.legend()\nplt.show()\n```\n\nIn this implementation, we load the Iris dataset, split it into training and testing sets, train a linear SVM classifier, predict the labels for the testing set, compute the accuracy of the model, and plot the decision boundaries using the first two features of the dataset. We also vary the value of the regularization parameter C to observe its effect on the performance of the model.\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}