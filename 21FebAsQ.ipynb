{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n\n**Web scraping** is the process of extracting data from websites. It involves retrieving and parsing HTML content to extract useful information. Web scraping is used to gather data from websites that do not provide APIs or structured data formats. It is commonly used in areas such as:\n\n1. **Data Collection:** Gathering data for research purposes, market analysis, or business intelligence.\n2. **Price Monitoring:** Scraping e-commerce websites to track product prices and analyze market trends.\n3. **Content Aggregation:** Aggregating news articles, blog posts, or social media content from various sources.\n\n### Q2. What are the different methods used for Web Scraping?\n\nThere are various methods used for web scraping, including:\n\n1. **Manual Scraping:** Copy-pasting data from websites manually.\n2. **Using Libraries:** Utilizing web scraping libraries and frameworks such as Beautiful Soup, Scrapy, and Selenium.\n3. **APIs:** Using APIs provided by websites (if available) to access structured data.\n4. **Browser Extensions:** Using browser extensions like Octoparse or DataMiner to scrape data visually.\n\n### Q3. What is Beautiful Soup? Why is it used?\n\n**Beautiful Soup** is a Python library used for web scraping. It provides tools for parsing HTML and XML documents, navigating the parse tree, and extracting data from web pages. Beautiful Soup is used because it simplifies the process of scraping web content by providing easy-to-use methods for accessing and manipulating HTML elements.\n\n### Q4. Why is Flask used in this Web Scraping project?\n\nFlask is a lightweight and flexible web framework for Python. It is commonly used in web scraping projects because it allows developers to quickly build and deploy web applications. Flask can be used to create a web interface for running and managing web scraping scripts, displaying scraped data, and providing APIs for accessing scraped data.\n\n### Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n\nIn a web scraping project hosted on AWS, you might use the following services:\n\n1. **EC2 (Elastic Compute Cloud):** EC2 is used to provision virtual servers (instances) to run web scraping scripts. It provides scalable computing capacity and allows you to choose instance types based on your requirements.\n\n2. **S3 (Simple Storage Service):** S3 is used to store scraped data, HTML content, or any other files generated during web scraping. It offers scalable and durable object storage with options for versioning, encryption, and lifecycle policies.\n\n3. **Lambda:** Lambda can be used for serverless execution of web scraping tasks. You can run Python scripts or functions in response to events triggered by API calls, file uploads to S3, or scheduled tasks using CloudWatch Events.\n\n4. **CloudWatch:** CloudWatch can be used for monitoring and logging web scraping activities. You can set up alarms, monitor performance metrics, and view logs to debug issues in your scraping scripts.\n\n5. **IAM (Identity and Access Management):** IAM is used to manage access permissions and roles for AWS resources. It allows you to control who can access EC2 instances, S3 buckets, Lambda functions, and other services used in your web scraping project.\n\nThese AWS services can be combined to create a scalable, reliable, and cost-effective infrastructure for web scraping applications.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}