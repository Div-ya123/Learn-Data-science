{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\nrepresent?\nQ2. Define adjusted R-squared and explain how it differs from the regular R-squared.\nQ3. When is it more appropriate to use adjusted R-squared?\nQ4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\ncalculated, and what do they represent?\nQ5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\nregression analysis.\nQ6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\nit more appropriate to use?\nQ7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\nexample to illustrate.\nQ8. Discuss the limitations of regularized linear models and explain why they may not always be the best\nchoice for regression analysis.\nQ9. You are comparing the performance of two regression models using different evaluation metrics.\nModel A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\nperformer, and why? Are there any limitations to your choice of metric?\nQ10. You are comparing the performance of two regularized linear models using different types of\nregularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\nuses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\nbetter performer, and why? Are there any trade-offs or limitations to your choice of regularization\nmethod?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n**Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?**\n\nR-squared (RÂ²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables in a linear regression model. It ranges from 0 to 1, where 0 indicates that the model explains none of the variability of the response data around its mean, and 1 indicates that the model explains all the variability of the response data around its mean. \n\nMathematically, R-squared is calculated as:\n\n\\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n\nWhere \\(SS_{res}\\) is the sum of squared residuals (the difference between the observed values and the predicted values), and \\(SS_{tot}\\) is the total sum of squares (the difference between each observed value and the mean of the dependent variable).\n\n**Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.**\n\nAdjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of irrelevant predictors to the model. Adjusted R-squared increases only if the new term improves the model more than would be expected by chance.\n\nMathematically, adjusted R-squared is calculated as:\n\n\\[ \\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\]\n\nWhere \\( n \\) is the number of observations and \\( k \\) is the number of predictors in the model.\n\n**Q3. When is it more appropriate to use adjusted R-squared?**\n\nAdjusted R-squared is more appropriate when comparing models with different numbers of predictors. It provides a more accurate measure of the goodness of fit of the model by penalizing the inclusion of unnecessary predictors.\n\n**Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?**\n\n- RMSE (Root Mean Squared Error): It is the square root of the average of the squared differences between predicted and actual values. It represents the square root of the variance of the residuals. Mathematically:\n\n\\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2} \\]\n\n- MSE (Mean Squared Error): It is the average of the squared differences between predicted and actual values. It represents the variance of the residuals. Mathematically:\n\n\\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 \\]\n\n- MAE (Mean Absolute Error): It is the average of the absolute differences between predicted and actual values. It represents the average magnitude of the errors in the predictions. Mathematically:\n\n\\[ MAE = \\frac{1}{n} \\sum_{i=1}^{n}|y_i - \\hat{y_i}| \\]\n\n**Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.**\n\nAdvantages:\n- RMSE and MSE give higher penalties to large errors, making them more sensitive to outliers.\n- RMSE and MSE are differentiable, which can be useful in optimization algorithms.\n- MAE is more robust to outliers compared to RMSE and MSE.\n\nDisadvantages:\n- RMSE and MSE are influenced more by large errors, which may not always be desirable.\n- MAE does not differentiate between large and small errors.\n- RMSE and MSE are sensitive to the scale of the data.\n\n**Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?**\n\nLasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to penalize the absolute size of the coefficients. It adds a penalty term to the least squares objective, forcing the sum of the absolute values of the coefficients to be less than a fixed value.\n\nLasso regularization differs from Ridge regularization in the type of penalty it imposes. Ridge regularization adds a penalty term proportional to the square of the coefficients, while Lasso adds a penalty term proportional to the absolute value of the coefficients.\n\nLasso regularization is more appropriate when there is a need for feature selection, as it tends to shrink the coefficients of less important features to zero, effectively removing them from the model.\n\n**Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.**\n\nRegularized linear models penalize the coefficients of the features, preventing them from taking excessively large values. This helps to simplify the model and reduces the chance of overfitting, where the model captures noise in the training data rather than the underlying relationship.\n\nFor example, in Lasso regularization, some coefficients may be shrunk to zero, effectively removing those features from the model. This simplification prevents the model from fitting too closely to the training data and improves its generalization to unseen data.\n\n**Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.**\n\nLimitations:\n- Regularized linear models assume linearity between the features and the target variable, which may not always hold true.\n- The choice of the regularization parameter is crucial, and it may not always be straightforward to select the optimal value.\n- Regularization can introduce bias into the model, particularly if the regularization parameter is too high.\n\nRegularized linear models may not always be the best choice for regression analysis when:\n- There are nonlinear relationships between the features and the target variable.\n- Interpretability of coefficients is important, as regularization can make the interpretation more complex.\n- The dataset is small, as regularization may lead to underfitting if there is not enough data to estimate the coefficients accurately.\n\n**Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?**\n\nThe choice depends on the specific requirements of the problem. If we want to penalize larger errors more heavily, we might prefer Model A with RMSE of 10. However, if we prefer a metric that is more robust to outliers, we might prefer Model B with MAE of 8. \n\nOne limitation is that RMSE and MAE measure different aspects of the error distribution, so the choice may depend on the specific characteristics of the data and the problem.\n\n**Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?**\n\nThe choice depends on the specific requirements of the problem. Ridge regularization tends to shrink the coefficients towards zero without necessarily setting them to zero, while Lasso regularization can lead to sparse solutions by",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}