{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "You are a data scientist working for a healthcare company, and you have been tasked with creating a\ndecision tree to help identify patients with diabetes based on a set of clinical variables. You have been\ngiven a dataset (diabetes.csv) with the following variables:\n1. Pregnancies: Number of times pregnant (integer)\n2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (integer)\n3. BloodPressure: Diastolic blood pressure (mm Hg) (integer)\n4. SkinThickness: Triceps skin fold thickness (mm) (integer)\n5. Insulin: 2-Hour serum insulin (mu U/ml) (integer)\n6. BMI: Body mass index (weight in kg/(height in m)^2) (float)\n7. DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes\nbased on family history) (float)\n8. Age: Age in years (integer)\n9. Outcome: Class variable (0 if non-diabetic, 1 if diabetic) (integer)\n\nHere’s the dataset link:\n\nYour goal is to create a decision tree to predict whether a patient has diabetes based on the other\nvariables. Here are the steps you can follow:\n\nhttps://drive.google.com/file/d/1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2/view?\n\nusp=sharing\n\nQ1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to\nunderstand the distribution and relationships between the variables.\nQ2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical\nvariables into dummy variables if necessary.\nQ3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility.\nQ4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use\ncross-validation to optimize the hyperparameters and avoid overfitting.\nQ5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy,\nprecision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results.\nQ6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important\nvariables and their thresholds. Use domain knowledge and common sense to explain the patterns and\ntrends.\nQ7. Validate the decision tree model by applying it to new data or testing its robustness to changes in the\ndataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and\nrisks.\nHere’s the dataset link:\n\nYour goal is to create a decision tree to predict whether a patient has diabetes based on the other\nvariables. Here are the steps you can follow:\n\nhttps://drive.google.com/file/d/1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2/view?\n\nusp=sharing\n\nBy following these steps, you can develop a comprehensive understanding of decision tree modeling and\nits applications to real-world healthcare problems. Good luck!",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n**Q1. Import the dataset and examine the variables:**\n\nTo begin, let's load the dataset and take a look at its structure and summary statistics. We'll use Python with Pandas for data manipulation and Matplotlib/Seaborn for visualization. \n\n```python\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nurl = \"https://drive.google.com/uc?id=1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2\"\ndiabetes_df = pd.read_csv(url)\n\n# Display the first few rows of the dataset\nprint(diabetes_df.head())\n\n# Get summary statistics of the dataset\nprint(diabetes_df.describe())\n\n# Check for missing values\nprint(diabetes_df.isnull().sum())\n\n# Visualize the distributions of numeric variables\nsns.pairplot(diabetes_df, hue='Outcome')\nplt.show()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(diabetes_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n```\n\nThis code will load the dataset, display the first few rows, show summary statistics, check for missing values, and visualize the distributions and relationships between variables using pair plots and a correlation matrix.\n\n\n\n\n**1. Importing the necessary libraries:**\n   - We start by importing the libraries that we'll use for data analysis and visualization. In this case, we're using Pandas for data manipulation and Matplotlib/Seaborn for visualization.\n\n**2. Loading the dataset:**\n   - We use Pandas' `read_csv()` function to load the dataset from the provided URL into a DataFrame called `diabetes_df`.\n\n**3. Displaying the first few rows of the dataset:**\n   - We use `head()` function to display the first few rows of the dataset. This gives us a quick look at the structure of the data and the values of each variable.\n\n**4. Getting summary statistics:**\n   - We use `describe()` function to get summary statistics of the dataset, such as count, mean, standard deviation, minimum, 25th percentile, median (50th percentile), 75th percentile, and maximum values for each numeric variable. This helps us understand the distribution of the data and identify potential outliers.\n\n**5. Checking for missing values:**\n   - We use `isnull().sum()` to check for missing values in the dataset. This gives us the count of missing values for each variable. Handling missing values is important because they can affect the performance of our models.\n\n**6. Visualizing the distributions of numeric variables:**\n   - We use `pairplot()` from Seaborn to create pair plots for each pair of numeric variables in the dataset. This allows us to visualize the distributions and relationships between variables. The `hue='Outcome'` parameter colors the data points based on the outcome variable, which helps us see if there are any patterns or differences between diabetic and non-diabetic patients.\n\n**7. Visualizing the correlation matrix:**\n   - We use `heatmap()` from Seaborn to create a heatmap of the correlation matrix between numeric variables. This helps us identify correlations between variables. High correlations (positive or negative) indicate strong relationships between variables, which can be important for feature selection and model interpretation.\n\nEach of these steps provides valuable insights into the dataset, helping us understand its structure, distribution, and relationships between variables. These insights will inform our preprocessing and modeling steps in subsequent parts of the analysis. If you have any questions about specific parts or need further clarification, feel free to ask!\n\nLet's continue with the next steps:\n\n**Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical variables into dummy variables if necessary:**\n\n```python\n# Importing necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\nurl = \"https://drive.google.com/uc?id=1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2\"\ndiabetes_df = pd.read_csv(url)\n\n# Handling missing values\n# Replace 0 values in certain columns with NaN (e.g., Glucose, BloodPressure, SkinThickness, Insulin, BMI)\ndiabetes_df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = \\\n    diabetes_df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)\n\n# Impute missing values using mean or median\ndiabetes_df.fillna(diabetes_df.median(), inplace=True)\n\n# Removing outliers (optional step)\n# You can use Z-score or IQR method to detect and remove outliers\n\n# Transforming categorical variables into dummy variables (if necessary)\n# There are no categorical variables in this dataset\n\n# Splitting the dataset into features and target variable\nX = diabetes_df.drop('Outcome', axis=1)\ny = diabetes_df['Outcome']\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Splitting the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n```\n\nIn this step:\n- We handle missing values by replacing 0 values in certain columns (e.g., Glucose, BloodPressure, SkinThickness, Insulin, BMI) with NaN and then imputing missing values using the median value of each column.\n- We can optionally remove outliers using techniques like Z-score or interquartile range (IQR) method.\n- Since there are no categorical variables in this dataset, we don't need to transform any variables into dummy variables.\n- We split the dataset into features (X) and the target variable (y).\n- We perform feature scaling using StandardScaler to standardize the features.\n- Finally, we split the dataset into training and test sets using a 80-20 split ratio.\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n**Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to understand the distribution and relationships between the variables:**\n\n```python\n# Importing necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = \"https://drive.google.com/uc?id=1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2\"\ndiabetes_df = pd.read_csv(url)\n\n# Display the first few rows of the dataset\nprint(diabetes_df.head())\n\n# Descriptive statistics\nprint(diabetes_df.describe())\n\n# Check for missing values\nprint(diabetes_df.isnull().sum())\n\n# Visualize the distributions of numeric variables\nsns.pairplot(diabetes_df, hue='Outcome')\nplt.show()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(diabetes_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n```\n\n**Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical variables into dummy variables if necessary:**\n\n```python\n# Handling missing values\ndiabetes_df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = \\\n    diabetes_df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)\ndiabetes_df.fillna(diabetes_df.median(), inplace=True)\n\n# Removing outliers (optional step)\n# You can use Z-score or IQR method to detect and remove outliers\n\n# Transforming categorical variables into dummy variables (if necessary)\n# There are no categorical variables in this dataset\n\n# Splitting the dataset into features and target variable\nX = diabetes_df.drop('Outcome', axis=1)\ny = diabetes_df['Outcome']\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Splitting the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n```\n\n**Q3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility:**\nThis was done as part of the preprocessing step, splitting the dataset into `X_train`, `X_test`, `y_train`, and `y_test` using `train_test_split` from scikit-learn, with a test size of 20% and a random seed of 42.\n\n**Q4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use cross-validation to optimize the hyperparameters and avoid overfitting:**\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the decision tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n# Define parameter grid for grid search\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Perform grid search with 5-fold cross-validation\ngrid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters:\", best_params)\n\n# Train the decision tree classifier with the best parameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n```\n\n**Q5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy, precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results:**\n```python\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix\n\n# Predictions on the test set\ny_pred = best_dt_classifier.predict(X_test)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# ROC curve and AUC score\ny_pred_proba = best_dt_classifier.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nauc_score = roc_auc_score(y_test, y_pred_proba)\n\n# Visualize confusion matrix\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Visualize ROC curve\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Print evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"AUC Score:\", auc_score)\n```\n\n**Q6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important variables and their thresholds. Use domain knowledge and common sense to explain the patterns and trends:**\n```python\n# Get feature importances\nfeature_importances = best_dt_classifier.feature_importances_\nfeature_names = X.columns\n\n# Sort feature importances in descending order\nsorted_indices = feature_importances.argsort()[::-1]\n\n# Print feature importances\nprint(\"Feature Importances:\")\nfor idx in sorted_indices:\n    print(f\"{feature_names[idx]}: {feature_importances[idx]}\")\n```\n\n**Q7. Validate the decision tree model by applying it to new data or testing its robustness to changes in the dataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and risks:**\nYou can apply the trained decision tree model to new data or test its robustness by performing sensitivity analysis or scenario testing. This involves testing the model's performance under different conditions or perturbations in the data.\n\nLet me know if you need further explanation on any part or if you have any other questions!",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}