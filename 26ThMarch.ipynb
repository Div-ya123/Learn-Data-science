{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\nexample of each.\nQ2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\na given dataset?\nQ3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\na real-world scenario.\nQ4. Explain the concept of gradient descent. How is it used in machine learning?\nQ5. Describe the multiple linear regression model. How does it differ from simple linear regression?\nQ6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\naddress this issue?\nQ7. Describe the polynomial regression model. How is it different from linear regression?\nQ8. What are the advantages and disadvantages of polynomial regression compared to linear\nregression? In what situations would you prefer to use polynomial regression?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\nQ1. **Simple Linear Regression vs. Multiple Linear Regression**\n- **Simple Linear Regression:** It involves predicting a dependent variable using only one independent variable. For example, predicting house prices (dependent variable) based on square footage (independent variable).\n- **Multiple Linear Regression:** It involves predicting a dependent variable using multiple independent variables. For example, predicting house prices based on square footage, number of bedrooms, and location.\n\nQ2. **Assumptions of Linear Regression**\n- Linearity: The relationship between the dependent and independent variables is linear.\n- Independence: The observations are independent of each other.\n- Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n- Normality: The residuals follow a normal distribution.\n- You can check these assumptions using visualizations like scatter plots for linearity, residual plots for homoscedasticity, and Q-Q plots for normality. Statistical tests like Durbin-Watson test can assess autocorrelation.\n\nQ3. **Interpretation of Slope and Intercept**\n- **Slope:** It represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.\n- **Intercept:** It represents the value of the dependent variable when all independent variables are zero.\n- For example, in a house price prediction model, the slope of square footage indicates how much the price increases for each additional square foot, while the intercept represents the base price of a house with zero square footage.\n\nQ4. **Gradient Descent in Machine Learning**\n- Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It involves iteratively adjusting model parameters (weights) in the direction of steepest descent of the cost function.\n- By repeatedly updating weights based on the gradient of the cost function, gradient descent helps the model converge to the optimal set of parameters for better predictions.\n\nQ5. **Multiple Linear Regression Model**\n- Multiple linear regression extends simple linear regression to include multiple independent variables. It predicts the dependent variable using a linear combination of these variables, each with its own coefficient.\n- The main difference is that multiple linear regression considers the combined effect of multiple independent variables on the dependent variable, providing a more comprehensive analysis compared to simple linear regression.\n\nQ6. **Multicollinearity in Multiple Linear Regression**\n- Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated with each other. It can lead to unstable coefficient estimates and difficulties in interpreting the model.\n- You can detect multicollinearity using correlation matrices or variance inflation factor (VIF). To address multicollinearity, you can remove highly correlated variables, combine variables, or use regularization techniques like ridge regression.\n\nQ7. **Polynomial Regression Model**\n- Polynomial regression is a form of regression analysis where the relationship between the dependent and independent variables is modeled as an nth degree polynomial.\n- Unlike linear regression, which assumes a linear relationship, polynomial regression can capture non-linear relationships between variables.\n\nQ8. **Advantages and Disadvantages of Polynomial Regression**\n- **Advantages:** It can model complex relationships and capture non-linear patterns in the data. It provides more flexibility compared to linear regression.\n- **Disadvantages:** It can lead to overfitting if the degree of the polynomial is too high. It may also be more computationally intensive and harder to interpret compared to linear regression.\n- Polynomial regression is preferred when the relationship between variables is non-linear and requires a more flexible model to fit the data accurately. However, caution should be taken to avoid overfitting.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}